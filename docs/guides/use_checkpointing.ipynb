{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9134fa",
   "metadata": {
    "id": "6e9134fa"
   },
   "source": [
    "# Save and load checkpoints\n",
    "\n",
    "In this guide, you will learn about saving and loading your Flax checkpoints with [Orbax](https://github.com/google/orbax).\n",
    "\n",
    "Orbax provides a variety of features related to saving and loading model data, which this guide will showcase:\n",
    "\n",
    "*  Support to various array types and storage formats;\n",
    "*  Asynchronous saving to reduce training wait time;\n",
    "*  Versioning and automatic bookkeeping of past checkpoints;\n",
    "*  Flexible [`transformations`](https://github.com/google/orbax/blob/main/docs/checkpoint.md#transformations) to tweak and load old checkpoints;\n",
    "*  [`jax.sharding`](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)-based API to save and load in multi-host scenarios.\n",
    "\n",
    "For backward-compatibility purposes, this guide will also show the equivalent calls in Flax legacy checkpointing APIs `flax.training.checkpoints`.\n",
    "\n",
    "See [Orbax Checkpoint Documentation](https://github.com/google/orbax/blob/main/docs/checkpoint.md) for more details.\n",
    "\n",
    "---\n",
    "**_Ongoing Migration:_** \n",
    "\n",
    "After July 30 2023, Flax's legacy checkpointing APIs `flax.training.checkpoints` will enter maintainence mode in favor of [Orbax](https://github.com/google/orbax).\n",
    "\n",
    "*  **If you are a new Flax user**: Please start with the new `orbax.checkpoint` API, as demonstrated in this guide. \n",
    "\n",
    "*  **If you have used legacy Flax APIs `flax.training.checkpoints`**: You have two options to consider:\n",
    "\n",
    "   * **Recommended**: Migrate your API calls to `orbax.checkpoint` API following this simple [migration guide](https://flax.readthedocs.io/en/latest/guides/orbax_upgrade_guide.html).\n",
    "\n",
    "   * **Transitory**: Try adding `flax.config.update('flax_use_orbax_checkpointing', True)` to your project, which will let your `flax.training.checkpoints` calls automatically use the Orbax backend to save your checkpoints. \n",
    "   \n",
    "      * **Special conditions may apply - check out the [Orbax-as-backend troubleshoot section](#orbax-as-backend-troubleshoot) for any issues.** \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f6aae",
   "metadata": {
    "id": "5a2f6aae"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install/upgrade Flax and [Orbax](https://github.com/google/orbax). For JAX installation with GPU/TPU support, visit [this section on GitHub](https://github.com/google/jax#installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80f8743",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# replace with `pip install flax` after release 0.6.9.\n",
    "! pip install -U -qq \"git+https://github.com/google/flax.git@main#egg=flax\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-icO30rwmKYj",
   "metadata": {
    "id": "-icO30rwmKYj"
   },
   "source": [
    "Note: Before running `import jax`, create eight fake devices to mimic [multi-host environment](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html?#aside-hosts-and-devices-in-jax) this notebook. Note that the order of imports is important here. The `os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'` command works only with the CPU backend. This means it won't work with GPU/TPU acceleration on if you're running this notebook in Google Colab. If you are already running the code on multiple devices (for example, in a 4x2 TPU environment), you can skip running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ArKLnsyGRxGv",
   "metadata": {
    "id": "ArKLnsyGRxGv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SJT9DTxTytjn",
   "metadata": {
    "id": "SJT9DTxTytjn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Any\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import checkpoints, train_state\n",
    "from flax import struct, serialization\n",
    "import orbax.checkpoint\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd6db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = 'tmp'\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)  # Remove any existing checkpoints from the last notebook run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d434cd",
   "metadata": {
    "id": "40d434cd"
   },
   "source": [
    "## Save checkpoints\n",
    "\n",
    "In Orbax and Flax, you can save and load any given JAX [pytree](https://jax.readthedocs.io/en/latest/pytrees.html). This includes not only typical Python and NumPy containers, but also customized classes extended from [`flax.struct.dataclass`](https://flax.readthedocs.io/en/latest/api_reference/flax.struct.html#flax.struct.dataclass). That means you can store almost any data generated â€” not only your model parameters, but any arrays/dictionaries, metadata/configs, and so on.\n",
    "\n",
    "Create a pytree with many data structures and containers, and play with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56dec3f6",
   "metadata": {
    "id": "56dec3f6",
    "outputId": "f1856d96-1961-48ed-bb7c-cb63fbaa7567"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': TrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: Array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: Array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState())),\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [Array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],      dtype=float32)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple model with one linear layer.\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x1 = random.normal(key1, (5,))      # A simple JAX array.\n",
    "model = nn.Dense(features=3)\n",
    "variables = model.init(key2, x1)\n",
    "\n",
    "# Flax's TrainState is a pytree dataclass and is supported in checkpointing.\n",
    "# Define your class with `@flax.struct.dataclass` decorator to make it compatible.\n",
    "tx = optax.sgd(learning_rate=0.001)      # An Optax SGD optimizer.\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=tx)\n",
    "# Perform a simple gradient update similar to the one during a normal training workflow.\n",
    "state = state.apply_gradients(grads=jax.tree_map(jnp.ones_like, state.params))\n",
    "\n",
    "# Some arbitrary nested pytree with a dictionary, a string, and a NumPy array.\n",
    "config = {'dimensions': np.array([5, 3]), 'name': 'dense'}\n",
    "\n",
    "# Bundle everything together.\n",
    "ckpt = {'model': state, 'config': config, 'data': [x1]}\n",
    "ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc59dfa",
   "metadata": {
    "id": "6fc59dfa"
   },
   "source": [
    "Now save the checkpoint with an Orbax `PyTreeCheckpointer`, directly to `tmp/orbax/single_save` directory.\n",
    "\n",
    "Note that an optional `save_args` is provided. This is recommended for performance speedups, as it bundles smaller arrays in your pytree to a single large file instead of multiple smaller files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b12da2",
   "metadata": {
    "id": "0pp4QtEqW9k7"
   },
   "outputs": [],
   "source": [
    "from flax.training import orbax_utils\n",
    "\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "orbax_checkpointer.save('tmp/orbax/single_save', ckpt, save_args=save_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4de1a",
   "metadata": {},
   "source": [
    "To use versioning and automatic bookkeeping features, you need to wrap an Orbax `CheckpointManager` over the `PyTreeCheckpointer`. \n",
    "\n",
    "Also provide an `CheckpointManagerOptions` that customizes your needs, such as how often and on what criteria you prefer old checkpoints be deleted. See [documentation](https://github.com/google/orbax/blob/main/docs/checkpoint.md#checkpointmanager) for a full list of options offered.\n",
    "\n",
    "`CheckpointManager` should be placed at top-level outside your training steps, to overall manage your saves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3686ea5",
   "metadata": {
    "id": "T6T8V4UBXB1R",
    "outputId": "b7132933-566d-440d-c34e-c5468d87cbdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\n",
    "checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
    "    'tmp/orbax/managed', orbax_checkpointer, options)\n",
    "\n",
    "# Inside a training loop\n",
    "for step in range(5):\n",
    "    # ... do your training ...\n",
    "    checkpoint_manager.save(step, ckpt, save_kwargs={'save_args': save_args})\n",
    "\n",
    "os.listdir('tmp/orbax/managed')  # since max_to_keep=2, only step 3 and 4 are retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbc4cc",
   "metadata": {
    "id": "OQkUOkHVW_4e"
   },
   "source": [
    "Alternatively you could save with the legacy Flax checkpointing utilities. Note that this provides less management features than Orbax's `CheckpointManagerOptions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdb35ef",
   "metadata": {
    "id": "4cdb35ef",
    "outputId": "6d849273-15ce-4480-8864-726d1838ac1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/flax-checkpointing/checkpoint_0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Flax Checkpoints.\n",
    "from flax.training import checkpoints\n",
    "\n",
    "checkpoints.save_checkpoint(ckpt_dir='tmp/flax-checkpointing',\n",
    "                            target=ckpt,\n",
    "                            step=0,\n",
    "                            overwrite=True,\n",
    "                            keep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b658bd1",
   "metadata": {
    "id": "6b658bd1"
   },
   "source": [
    "## Restore checkpoints\n",
    "\n",
    "In Orbax, call `.restore()` for either `PyTreeCheckpointer` or `CheckpointManager` will restore your checkpoint, in the raw pytree format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a807a9c1",
   "metadata": {
    "id": "WgRJj3wjXIaN",
    "outputId": "b4af1ef4-f22f-459b-bdca-2e6bfa16c08b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)],\n",
       " 'model': {'opt_state': [None, None],\n",
       "  'params': {'bias': array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "   'kernel': array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "          [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "          [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "          [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "          [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32)},\n",
       "  'step': 1}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_restored = orbax_checkpointer.restore('tmp/orbax/single_save')\n",
    "raw_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c015a22",
   "metadata": {},
   "source": [
    "Note that the `step` number is required for `CheckpointManger`. You could also use `.latest_step()` to find the latest step available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "251d7085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)],\n",
       " 'model': {'opt_state': [None, None],\n",
       "  'params': {'bias': array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "   'kernel': array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "          [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "          [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "          [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "          [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32)},\n",
       "  'step': 1}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = checkpoint_manager.latest_step()  # step = 4\n",
    "checkpoint_manager.restore(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe3bc8",
   "metadata": {
    "id": "VKJrfSyLXGrc"
   },
   "source": [
    "Alternatively you could load the checkpoint with the legacy Flax checkpointing utilities.\n",
    "\n",
    "Note that With the migration to Orbax in progress, `restore_checkpoint` can automatically identify whether a checkpoint is saved in the legacy Flax format or with an Orbax backend, and restore the pytree correctly. Therefore, adding `flax.config.update('flax_use_orbax_checkpointing', True)` won't hurt your ability to restore old checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "150b20a0",
   "metadata": {
    "id": "150b20a0",
    "outputId": "85ffceca-f38d-46b8-e567-d9d38b7885f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'step': 1,\n",
       "  'params': {'bias': array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "   'kernel': array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "          [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "          [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "          [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "          [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32)},\n",
       "  'opt_state': {'0': {}, '1': {}}},\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': {'0': array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_restored = checkpoints.restore_checkpoint(ckpt_dir='tmp/flax-checkpointing', target=None)\n",
    "raw_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b981f",
   "metadata": {
    "id": "987b981f"
   },
   "source": [
    "## Restore with custom dataclasses\n",
    "\n",
    "Note that all the pytrees restored above are raw dictionaries, whereas the original pytrees contain custom dataclasses like `TrainState` and `optax` states. That is because when restoring the pytree, the program does not yet know which structure it once belongs to.\n",
    "\n",
    "To resolve this, you should provide an example pytree to let the Orbax or Flax know exactly what structure it should restore to. This example should introduce any custom Flax dataclasses explicitly, and have the same structure as the saved checkpoint.\n",
    "\n",
    "> Side note: Data that was a JAX NumPy array (`jnp.array`) will be restored as a NumPy array (`numpy.array`). This would not affect your work because JAX will [automatically convert](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html) NumPy arrays to JAX arrays once the computation starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f42513",
   "metadata": {
    "id": "58f42513",
    "outputId": "110c6b6e-fe42-4179-e5d8-6b92d355e11b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)],\n",
       " 'model': TrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState()))}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=jax.tree_map(np.zeros_like, variables['params']),  # values of the tree leaf doesn't matter\n",
    "    tx=tx,\n",
    ")\n",
    "empty_config = {'dimensions': np.array([0, 0]), 'name': ''}\n",
    "target = {'model': empty_state, 'config': empty_config, 'data': [jnp.zeros_like(x1)]}\n",
    "state_restored = orbax_checkpointer.restore('tmp/orbax/single_save', item=target)\n",
    "state_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c18bc6",
   "metadata": {},
   "source": [
    "Alternatively, restore from Orbax `CheckpointManager` and from legacy Flax code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61e9a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)],\n",
       " 'model': TrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState()))}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_manager.restore(4, items=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "412af50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': TrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState())),\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints.restore_checkpoint(ckpt_dir='tmp/flax-checkpointing', target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27461ac8",
   "metadata": {},
   "source": [
    "It's often recommended to refactor out the process of initializing a checkpoint's structure (for example, a [`TrainState`](https://flax.readthedocs.io/en/latest/flip/1009-optimizer-api.html?#train-state)), so that saving/loading is easier and less error-prone. This is because functions and complex objects like `apply_fn` and `tx` (optimizer) cannot be serialized into the checkpoint file and must be initiated by code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a300a",
   "metadata": {
    "id": "136a300a"
   },
   "source": [
    "### Restore when checkpoint structures differ\n",
    "\n",
    "During your development, at times you change your model, or you add and remove fields to tweak it better for your needs. That means your checkpoint structure will change! How can your old data be loaded to your new code?\n",
    "\n",
    "Below showcases a simple example - a `TrainStatePlus` extended from `TrainState` that contains an extra field `batch_stats`. In real practice, you may need this when doing [batch normalization](https://flax.readthedocs.io/en/latest/guides/batch_norm.html).\n",
    "\n",
    "Let's store the new `TrainStatePlus` as step 5. Remember that step 4 has the old `TrainState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be65d4af",
   "metadata": {
    "id": "be65d4af",
    "outputId": "4fe776f0-65f8-4fc4-d64a-990520b36dce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomTrainState(train_state.TrainState):\n",
    "    batch_stats: Any = None\n",
    "\n",
    "custom_state = CustomTrainState.create(\n",
    "    apply_fn=state.apply_fn,\n",
    "    params=state.params,\n",
    "    tx=state.tx,\n",
    "    batch_stats=np.arange(10),\n",
    ")\n",
    "\n",
    "custom_ckpt = {'model': custom_state, 'config': config, 'data': [x1]}\n",
    "# Use a custom state to read the old `TrainState` checkpoint.\n",
    "custom_target = {'model': custom_state, 'config': None, 'data': [jnp.zeros_like(x1)]}\n",
    "\n",
    "\n",
    "# Save it in Orbax.\n",
    "custom_save_args = orbax_utils.save_args_from_target(custom_ckpt)\n",
    "checkpoint_manager.save(5, custom_ckpt, save_kwargs={'save_args': custom_save_args})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c2255",
   "metadata": {
    "id": "379c2255"
   },
   "source": [
    "It is recommended to keep your checkpoints up to date with your pytree dataclass definitions, but things happens and you might be forced to restore checkpoints with incompatible reference objects at runtime. \n",
    "\n",
    "When this happens, the checkpoint restoration will try to respect the structure of the reference when given. Below showcases a few common scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa9652",
   "metadata": {},
   "source": [
    "#### When reference object is partial\n",
    "\n",
    "If your reference object is a subtree of your checkpoint, the restoration will ignore the additional field(s) and restore a checkpoint with same structure as the reference. \n",
    "\n",
    "Like in the example below, the `batch_stats` field in `CustomTrainState` was ignored, and the checkpoint was restored as a `TrainState`.\n",
    "\n",
    "This can also be useful for reading only part of your checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68828029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)],\n",
       " 'model': TrainState(step=0, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState()))}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored = checkpoint_manager.restore(5, items=target)\n",
    "assert not hasattr(restored, 'batch_stats')\n",
    "assert type(restored['model']) == train_state.TrainState\n",
    "restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6822c6",
   "metadata": {},
   "source": [
    "#### When checkpoint is partial\n",
    "\n",
    "On the other hand, if the reference object contains a value that is not available in the checkpoint, the checkpointing code will by default warn that some data is not compatible.\n",
    "\n",
    "To bypass the error, you need to pass an Orbax [`transform`](https://github.com/google/orbax/blob/main/docs/checkpoint.md#transformations) that teaches Orbax how to conform this checkpoint into the structure of `custom_target`.\n",
    "\n",
    "In this case, we pass a default `{}` that lets Orbax use values in `custom_target` to fill in the blank. This lets us restore an old checkpoint into a new data structure, the `CustomTrainState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d14c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError when target state has an unmentioned field: 'batch_stats'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': None,\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)],\n",
       " 'model': CustomTrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    checkpoint_manager.restore(4, items=custom_target)\n",
    "except KeyError as e:\n",
    "    print(f'KeyError when target state has an unmentioned field: {e}')\n",
    "    print('')\n",
    "\n",
    "# Step 4 is an original TrainState, without \"batch_stats\"\n",
    "restored = checkpoint_manager.restore(4, items=custom_target, \n",
    "                                      restore_kwargs={'transforms': {}})\n",
    "assert type(restored['model']) == CustomTrainState\n",
    "np.testing.assert_equal(restored['model'].batch_stats, \n",
    "                        custom_target['model'].batch_stats)\n",
    "restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4b0fd",
   "metadata": {},
   "source": [
    "If you already saved your checkpoints with an Orbax backend, you can use `orbax_transforms` to access this `transforms` arg in the Flax API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29fd1e33",
   "metadata": {
    "id": "29fd1e33",
    "outputId": "cdbb9247-d1eb-4458-aa83-8db0332af7cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': CustomTrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),\n",
       " 'config': None,\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save in Flax-with-Orbax-backend.\n",
    "flax.config.update('flax_use_orbax_checkpointing', True)\n",
    "checkpoints.save_checkpoint(ckpt_dir='tmp/flax-checkpointing',\n",
    "                            target=ckpt,\n",
    "                            step=4,\n",
    "                            overwrite=True,\n",
    "                            keep=2)\n",
    "\n",
    "checkpoints.restore_checkpoint('tmp/flax-checkpointing', target=custom_target, step=4,\n",
    "                               orbax_transforms={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ef07c",
   "metadata": {},
   "source": [
    "In legacy Flax checkpoint APIs, similar things are doable too, but not as flexible as [Orbax Transformations](https://github.com/google/orbax/blob/main/docs/checkpoint.md#transformations). You need to first restore the checkpoint to a raw dict with `target=None`, modify the structure accordingly, and then deserialize it back to original target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051e7a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': CustomTrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])),\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save in legacy Flax.\n",
    "flax.config.update('flax_use_orbax_checkpointing', False)\n",
    "checkpoints.save_checkpoint(ckpt_dir='tmp/flax-checkpointing',\n",
    "                            target=ckpt,\n",
    "                            step=5,\n",
    "                            overwrite=True,\n",
    "                            keep=2)\n",
    "\n",
    "# Pass no target to get a raw state dictionary first.\n",
    "raw_state_dict = checkpoints.restore_checkpoint('tmp/flax-checkpointing', target=None, step=5)\n",
    "# Add/remove fields as needed.\n",
    "raw_state_dict['model']['batch_stats'] = np.flip(np.arange(10))\n",
    "# Restore the classes with correct target now\n",
    "flax.serialization.from_state_dict(custom_target, raw_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b39501",
   "metadata": {
    "id": "a6b39501"
   },
   "source": [
    "## Asynchronized checkpointing\n",
    "\n",
    "Checkpointing is I/O heavy, and if you have a large amount of data to save, it may be worthwhile to put it into a background thread, while continuing with your training.\n",
    "\n",
    "You can do this by creating an [`orbax.checkpoint.AsyncCheckpointer`](https://github.com/google/orbax/blob/main/orbax/checkpoint/async_checkpointer.py) in place of the `orbax.checkpoint.PyTreeCheckpointer`.\n",
    "\n",
    "Note: You should use the same `async_checkpointer` to handle all your async saves across your training steps, so that it can make sure that a previous async save is done before the next one begins. This enables bookkeeping, such as `keep` (the number of checkpoints) and `overwrite` to be consistent across steps.\n",
    "\n",
    "Whenever you want to explicitly wait until an async save is done, you can call `async_checkpointer.wait_until_finished()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85be68a6",
   "metadata": {
    "id": "85be68a6",
    "outputId": "aefce94c-8bae-4355-c142-05f2b61c39e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)],\n",
       " 'model': TrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = dot_general\n",
       " )>, params=FrozenDict({\n",
       "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
       "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
       "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
       "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
       "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
       "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc1c0182550>, update=<function chain.<locals>.update_fn at 0x7fc1c011fdc0>), opt_state=(EmptyState(), EmptyState()))}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `orbax.checkpoint.AsyncCheckpointer` needs some multi-process initialization, because it was\n",
    "# originally designed for multi-process large model checkpointing.\n",
    "# For Python notebooks or other single-process setting, just set up with `num_processes=1`.\n",
    "# Refer to https://jax.readthedocs.io/en/latest/multi_process.html#initializing-the-cluster\n",
    "# for how to set it up in multi-process scenarios.\n",
    "jax.distributed.initialize(\"localhost:8889\", num_processes=1, process_id=0)\n",
    "\n",
    "async_checkpointer = orbax.checkpoint.AsyncCheckpointer(\n",
    "    orbax.checkpoint.PyTreeCheckpointHandler(), timeout_secs=50)\n",
    "\n",
    "# Save your job:\n",
    "async_checkpointer.save('tmp/orbax/single_save_async', ckpt, save_args=save_args)\n",
    "# ... Continue with your work...\n",
    "\n",
    "# ... Until a time when you want to wait until the save completes:\n",
    "async_checkpointer.wait_until_finished()  # Blocks until the checkpoint saving is completed.\n",
    "async_checkpointer.restore('tmp/orbax/single_save_async', item=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e93db6",
   "metadata": {
    "id": "QpuTCeMVXOBn"
   },
   "source": [
    "If you are using Orbax `CheckpointManager`, just pass in the async_checkpointer when initializing it. Then, in practice, call `async_checkpoint_manager.wait_until_finished()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af33b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
    "    'tmp/orbax/managed_async', async_checkpointer, options)\n",
    "async_checkpoint_manager.wait_until_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e03cd",
   "metadata": {
    "id": "13e93db6"
   },
   "source": [
    "## Multi-host/multi-process checkpointing\n",
    "\n",
    "JAX provides a few ways to scale up your code on multiple hosts at the same time. This usually happens when the number of devices (CPU/GPU/TPU) is so large that different devices are managed by different hosts (CPU). To get started on JAX in multi-process settings, check out [Using JAX in multi-host and multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html) and the [distributed array guide](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html).\n",
    "\n",
    "In the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm with JAX [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html), a large multi-process array can have its data sharded across different devices (check out the `pjit` [JAX-101 tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html)). When a multi-process array is serialized, each host dumps its data shards to a single shared storage, such as a Google Cloud bucket.\n",
    "\n",
    "Orbax supports saving and loading pytrees with multi-process arrays in the same fashion as single-process pytrees. However, it's recommended to use the asynchronized [`orbax.AsyncCheckpointer`](https://github.com/google/orbax/blob/main/orbax/checkpoint/async_checkpointer.py) to save large multi-process arrays on another thread, so that you can perform computation alongside the saves. With pure Orbax, saving checkpoints in a multiprocess context uses the same API as in a single process context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ubdUvyMrhD-1",
   "metadata": {
    "id": "ubdUvyMrhD-1"
   },
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec, NamedSharding\n",
    "\n",
    "# Create an array sharded across multiple devices.\n",
    "mesh_shape = (4, 2)\n",
    "devices = np.asarray(jax.devices()).reshape(*mesh_shape)\n",
    "mesh = jax.sharding.Mesh(devices, ('x', 'y'))\n",
    "\n",
    "mp_array = jax.device_put(np.arange(8 * 2).reshape(8, 2),\n",
    "                          NamedSharding(mesh, PartitionSpec('x', 'y')))\n",
    "\n",
    "# Make it a pytree, just because we can.\n",
    "mp_ckpt = {'model': mp_array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a669bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_checkpoint_manager.save(0, mp_ckpt)\n",
    "async_checkpoint_manager.wait_until_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deee32e",
   "metadata": {},
   "source": [
    "When restoring a checkpoint with multi-process arrays, you need to specify what `sharding` each array should be restored back to. Otherwise, they will be restored as large `np.array`s on process 0, costing time and memory.\n",
    "\n",
    "(In this notebook, since we are on single-process, it will be restored as `np.array` even if we provide shardings.)\n",
    "\n",
    "Orbax allows you to specify this by passing a pytree of `sharding`s in `restore_args`. If you already have a reference pytree that has all the arrays with the right sharding, you can use `orbax_utils.restore_args_from_target` to transform it into the `restore_args` that Orbax needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8e7daaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': array([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15]], dtype=int32)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The reference doesn't need to be as large as your checkpoint!\n",
    "# Just make sure it has the `.sharding` you want.\n",
    "mp_smaller = jax.device_put(np.arange(8).reshape(4, 2),\n",
    "                            NamedSharding(mesh, PartitionSpec('x', 'y')))\n",
    "ref_ckpt = {'model': mp_smaller}\n",
    "\n",
    "restore_args = orbax_utils.restore_args_from_target(ref_ckpt)\n",
    "async_checkpoint_manager.restore(\n",
    "    0, items=ref_ckpt, restore_kwargs={'restore_args': restore_args})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc355ce",
   "metadata": {
    "id": "edc355ce"
   },
   "source": [
    "### Legacy Flax: use `save_checkpoint_multiprocess`\n",
    "\n",
    "In legacy Flax, to save multi-process arrays, use [`flax.training.checkpoints.save_checkpoint_multiprocess()`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint_multiprocess) in place of `save_checkpoint()` and with the same arguments.\n",
    "\n",
    "If your checkpoint is too large, you can specify `timeout_secs` in the manager and give it more time to finish writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d10039b",
   "metadata": {
    "id": "5d10039b",
    "outputId": "901bb097-0899-479d-b9ae-61dae79e7057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/checkpoint_3'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_checkpointer = orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler(), timeout_secs=50)\n",
    "checkpoints.save_checkpoint_multiprocess(ckpt_dir,\n",
    "                                         mp_ckpt,\n",
    "                                         step=3,\n",
    "                                         overwrite=True,\n",
    "                                         keep=4,\n",
    "                                         orbax_checkpointer=async_checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9f9724c",
   "metadata": {
    "id": "a9f9724c",
    "outputId": "393c4a0e-8a8c-4ca6-c609-93c8bab38e75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': array([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15]], dtype=int32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_restored = checkpoints.restore_checkpoint(ckpt_dir,\n",
    "                                             target=ref_ckpt,\n",
    "                                             step=3,\n",
    "                                             orbax_checkpointer=async_checkpointer)\n",
    "mp_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cfdd59",
   "metadata": {},
   "source": [
    "## Orbax-As-Backend Troubleshoot\n",
    "\n",
    "As an intermediate stage of the migration, `flax.training.checkpoints` APIs will start to use Orbax as their backend when saving checkpoints starting from May 15, 2023.\n",
    "\n",
    "Checkpoints saved with Orbax backend can be readable by either `flax.training.checkpoints.restore_checkpoint` or a `orbax.checkpoint.PyTreeCheckpointer`.\n",
    "\n",
    "Code-wise, this is equivalent to setting the config flag [`flax.config.flax_use_orbax_checkpointing`](https://github.com/google/flax/blob/main/flax/configurations.py#L103) default to `True`. You could overwrite this value in your project with `flax.config.update('flax_use_orbax_checkpointing', <BoolValue>)` at any time.\n",
    "\n",
    "In general, this automatic migration will not affect most users. However, you may encounter issues if your API usage follows some specific pattern. Check out the sections below for troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bceb1",
   "metadata": {},
   "source": [
    "### If your devices hangs when writing checkpoints\n",
    "\n",
    "If you are running in multi-host environment (usually anything larger than 8 TPU devices) and your devices hang when writing checkpoints, please check out if your code is in the following pattern (aka. `save_checkpoint` only ran on host 0):\n",
    "\n",
    "```\n",
    "if jax.process_index() == 0:\n",
    "  flax.training.checkpoints.save_checkpoint(...)\n",
    "```\n",
    "\n",
    "Unfortunately this is a legacy pattern that we plan to deprecate our support for, because in multi-process environment, the checkpointing code should coordinate among hosts instead of being triggered only on the host 0. Replacing the code above with the following should resolve the hang issue:\n",
    "\n",
    "```\n",
    "flax.training.checkpoints.save_checkpoint_multiprocess(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0ebb3",
   "metadata": {},
   "source": [
    "### If you don't save pytrees\n",
    "\n",
    "Orbax uses `PyTreeCheckpointHandler` to save checkpoints, which means they only save pytrees. If you want to save singular arrays or numbers, you have two options:\n",
    "\n",
    "1. Use `orbax.ArrayCheckpointHandler` to save them following [this migration section](https://flax.readthedocs.io/en/latest/guides/orbax_upgrade_guide.html#saving-loading-a-single-jax-or-numpy-array).\n",
    "\n",
    "1. Wrap it inside a pytree and save as usual."
   ]
  }
 ],
 "metadata": {
  "gpuClass": "standard",
  "jupytext": {
   "formats": "ipynb,md",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
